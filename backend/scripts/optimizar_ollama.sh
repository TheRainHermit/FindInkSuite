#!/bin/bash

echo "üöÄ OPTIMIZACI√ìN OLLAMA - DISTRIBUCI√ìN DE CARGA CPU"
echo "=================================================="
echo "‚úÖ 4 cores AMD PRO A10-8750B"
echo "‚úÖ 13GB RAM disponible"
echo "‚úÖ Configuraci√≥n multi-proceso"
echo "=================================================="

# Configuraci√≥n de variables de entorno para optimizar Ollama
export OLLAMA_NUM_PARALLEL=2          # M√°ximo 2 modelos simult√°neos
export OLLAMA_MAX_LOADED_MODELS=2     # Mantener 2 modelos en memoria
export OLLAMA_MAX_QUEUE=4             # Cola de m√°ximo 4 peticiones
export OLLAMA_FLASH_ATTENTION=1       # Usar flash attention para optimizar
export OLLAMA_HOST=127.0.0.1:11434    # Bind espec√≠fico

# Configurar n√∫cleos de CPU para Ollama (usar 3 de 4 cores)
export GOMAXPROCS=3

# Funci√≥n para configurar modelo espec√≠fico
configure_model() {
    local model=$1
    local threads=$2
    echo "üîß Configurando $model con $threads threads..."
    
    # Configurar el modelo con par√°metros optimizados
    ollama create ${model}_optimized -f - << EOF
FROM $model
PARAMETER num_thread $threads
PARAMETER num_gpu 0
PARAMETER mmap true
PARAMETER numa true
EOF
}

echo ""
echo "üîß PASO 1: Deteniendo Ollama actual..."
sudo pkill -f ollama
sleep 3

echo ""
echo "üîß PASO 2: Iniciando Ollama optimizado..."
# Iniciar Ollama en background con configuraci√≥n optimizada
nohup ollama serve > /tmp/ollama_optimized.log 2>&1 &
sleep 5

echo ""
echo "üîß PASO 3: Configurando modelos optimizados..."
# Configurar cada modelo con threads espec√≠ficos
configure_model "phi" 1          # Phi: 1 thread (m√°s r√°pido)
configure_model "mistral" 2      # Mistral: 2 threads (equilibrado)
configure_model "llama2:7b-chat" 2  # Llama2: 2 threads

echo ""
echo "üîß PASO 4: Pre-cargando modelo r√°pido..."
# Pre-cargar phi para respuestas inmediatas
echo '{"model": "phi", "prompt": "test", "stream": false}' | \
curl -s -X POST http://localhost:11434/api/generate \
     -H "Content-Type: application/json" \
     -d @- > /dev/null

echo ""
echo "‚úÖ OLLAMA OPTIMIZADO CONFIGURADO:"
echo "================================"
echo "üöÄ Parallelismo: 2 modelos simult√°neos"
echo "üß† RAM optimizada: 2 modelos en memoria"
echo "‚ö° Threads por modelo:"
echo "   - Phi: 1 thread (respuesta r√°pida)"
echo "   - Mistral: 2 threads (equilibrado)"
echo "   - Llama2: 2 threads (calidad)"
echo "üîÑ Cola de peticiones: 4 m√°ximo"
echo "üíæ Cache optimizado activado"
echo ""

echo "üß™ PROBANDO RENDIMIENTO..."
echo "========================="

# Test de velocidad
echo "‚è±Ô∏è Test Phi (deber√≠a ser <5 segundos):"
time curl -s -X POST http://localhost:11434/api/generate \
     -H "Content-Type: application/json" \
     -d '{"model": "phi", "prompt": "Hi", "stream": false}' | \
     jq '.response'

echo ""
echo "üéØ OPTIMIZACI√ìN COMPLETADA"
echo "=========================="
echo "‚úÖ Ollama optimizado para 4 cores"
echo "‚úÖ Distribuci√≥n de carga configurada"
echo "‚úÖ Modelos pre-configurados"
echo "‚úÖ Cache en RAM activado"
echo ""
echo "üí° Ahora reinicia tu servidor API para usar la configuraci√≥n optimizada"